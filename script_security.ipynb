{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divanshu1993/security-anomalies-samples/blob/main/script_security.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **My First ML project**"
      ],
      "metadata": {
        "id": "zNuklwACNZo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load the CSV data\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/divanshu1993/security-anomalies-samples/main/security%20anomalies%20threats%20samples.csv')\n",
        "\n",
        "# Step 2: Preprocess the data (if required)\n",
        "# For example, you may need to clean or normalize the text data\n",
        "\n",
        "# Step 3: Prepare the training data\n",
        "X_train = data['message'].str.lower().str.strip()\n",
        "y_train = data['status']\n",
        "\n",
        "# Step 4: Vectorize the training data\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Step 5: Train the SVM model\n",
        "svm = SVC()\n",
        "svm.fit(X_train_vec, y_train)\n",
        "\n",
        "# Step 6: Save the trained model and vectorizer for future use\n",
        "joblib.dump(svm, 'anomaly_detection_model.pkl')\n",
        "joblib.dump(vectorizer, 'vectorizer.pkl')"
      ],
      "metadata": {
        "id": "-tP6RwFomNj2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cecc1727-404f-4894-a443-419ae5a468bf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['vectorizer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test My New Generated Model**"
      ],
      "metadata": {
        "id": "aFyLsP8aiji1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required libraries\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Step 2: Import the necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import joblib\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Step 3: Load the trained model and vectorizer\n",
        "vectorizer = joblib.load('vectorizer.pkl')\n",
        "svm = joblib.load('anomaly_detection_model.pkl')\n",
        "\n",
        "# Step 4: Load and preprocess the test data\n",
        "test_data = pd.read_csv('https://raw.githubusercontent.com/divanshu1993/security-anomalies-samples/main/fake_logs_5000.csv')\n",
        "# Preprocessing steps (if required)\n",
        "\n",
        "# Step 5: Prepare the data for testing\n",
        "X_test = test_data['message'].str.lower().tolist()\n",
        "\n",
        "\n",
        "\n",
        "# Skip rows with null first name and last name\n",
        "print(\"len before data transformation:\", len(log_data))\n",
        "log_data = log_data[~(log_data['message'].astype(str).str.contains('First Name', case=False) & log_data['message'].astype(str).str.contains('null', case=False)) & ~(log_data['message'].astype(str).str.contains('Last Name', case=False) & log_data['message'].astype(str).str.contains('null', case=False))]\n",
        "print(\"len after data transformation:\", len(log_data))\n",
        "\n",
        "# Remove timestamp, [main], INFO, and file path from log messages\n",
        "cleaned_log_messages = []\n",
        "for log_message in log_messages:\n",
        "  cleaned_log_messages.append(log_message.split(\" - \")[1])\n",
        "\n",
        "\n",
        "# Step 6: Vectorize the new data using the loaded vectorizer\n",
        "X_new_vec = vectorizer.transform(cleaned_log_messages)\n",
        "\n",
        "# Step 7: Predict anomalies using the loaded model\n",
        "predictions = svm.predict(X_new_vec)\n",
        "\n",
        "\n",
        "# Step 8: Print the predictions\n",
        "print(\"Predictions:\", predictions)\n",
        "\n",
        "# Step 9: Calculate and print the F1-score\n",
        "true_labels = test_data['status']  # Replace 'true_label_column' with the actual column name of true labels\n",
        "print('labels : ', true_labels)\n",
        "report = classification_report(true_labels, predictions)\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDQiXwOXioDY",
        "outputId": "59771a0b-843b-4a37-d5d7-827e1e2c0953"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "len before data transformation: 3609\n",
            "len after data transformation: 3609\n",
            "Predictions: ['anomaly' 'anomaly' 'anomaly' ... 'anomaly' 'anomaly' 'anomaly']\n",
            "labels :  0       normal\n",
            "1       normal\n",
            "2       normal\n",
            "3       normal\n",
            "4       normal\n",
            "         ...  \n",
            "4995    normal\n",
            "4996    normal\n",
            "4997    normal\n",
            "4998    normal\n",
            "4999    normal\n",
            "Name: status, Length: 5000, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anomaly       0.22      1.00      0.36      1087\n",
            "      normal       0.00      0.00      0.00      3913\n",
            "\n",
            "    accuracy                           0.22      5000\n",
            "   macro avg       0.11      0.50      0.18      5000\n",
            "weighted avg       0.05      0.22      0.08      5000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Add User Data with Isolation Forest algo "
      ],
      "metadata": {
        "id": "WxiZrfrGYAtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Step 4: Load and preprocess the test data\n",
        "log_data = pd.read_csv('https://raw.githubusercontent.com/divanshu1993/security-anomalies-samples/main/fake_logs_5000.csv')\n",
        "# Preprocessing steps (if required)\n",
        "\n",
        "# Preprocess the log data and extract the log messages\n",
        "log_messages = log_data['message'].tolist()\n",
        "true_labels = log_data['status'].tolist()\n",
        "\n",
        "# Skip rows with null first name and last name\n",
        "print(\"len before data transformation:\", len(log_data))\n",
        "log_data = log_data[~(log_data['message'].astype(str).str.contains('First Name', case=False) & log_data['message'].astype(str).str.contains('null', case=False)) & ~(log_data['message'].astype(str).str.contains('Last Name', case=False) & log_data['message'].astype(str).str.contains('null', case=False))]\n",
        "print(\"len after data transformation:\", len(log_data))\n",
        "\n",
        "# Remove timestamp, [main], INFO, and file path from log messages\n",
        "cleaned_log_messages = []\n",
        "for log_message in log_messages:\n",
        "    cleaned_message = re.sub(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d+|\\[\\w+\\]|INFO|-\\s?[\\/\\w]+', '', log_message)\n",
        "    cleaned_log_messages.append(cleaned_message.strip())\n",
        "    \n",
        "# Convert log messages to TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(cleaned_log_messages)\n",
        "\n",
        "# Apply Isolation Forest for anomaly detection\n",
        "isolation_forest = IsolationForest(contamination=0.01)\n",
        "isolation_forest.fit(X)\n",
        "\n",
        "# Predict the anomaly scores for the log messages\n",
        "anomaly_scores = isolation_forest.decision_function(X)\n",
        "\n",
        "# Set a threshold for anomaly detection (you can adjust this as needed)\n",
        "threshold = 0\n",
        "\n",
        "# Determine the predicted labels (anomaly or normal)\n",
        "predicted_labels = [1 if score >= threshold else -1 for score in anomaly_scores]\n",
        "\n",
        "# Map true labels to numeric values (anomaly: -1, normal: 1)\n",
        "true_labels_numeric = [-1 if label == \"anomaly\" else 1 for label in true_labels]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_labels_numeric, predicted_labels)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(true_labels_numeric, predicted_labels, pos_label=-1)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(true_labels_numeric, predicted_labels, pos_label=-1)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(true_labels_numeric, predicted_labels, pos_label=-1)\n",
        "\n",
        "# Print the metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml8APTe3OhzT",
        "outputId": "aadd056e-de27-4b58-c525-9c30fba196fb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "len before data transformation: 5000\n",
            "len after data transformation: 3609\n",
            "Accuracy: 0.7862\n",
            "Precision: 0.68\n",
            "Recall: 0.031278748850046\n",
            "F1 score: 0.059806508355321024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing Add User Data with KNN algo to check prediction**"
      ],
      "metadata": {
        "id": "VyzgJMRedNGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Step 4: Load and preprocess the test data\n",
        "log_data = pd.read_csv('https://raw.githubusercontent.com/divanshu1993/security-anomalies-samples/main/fake_logs_5000.csv')\n",
        "# Preprocessing steps (if required)\n",
        "\n",
        "# Preprocess the log data and extract the log messages\n",
        "log_messages = log_data['message'].tolist()\n",
        "true_labels = log_data['status'].tolist()\n",
        "\n",
        "# Skip rows with null first name and last name\n",
        "print(\"len before data transformation:\", len(log_data))\n",
        "log_data = log_data[~(log_data['message'].astype(str).str.contains('First Name', case=False) & log_data['message'].astype(str).str.contains('null', case=False)) & ~(log_data['message'].astype(str).str.contains('Last Name', case=False) & log_data['message'].astype(str).str.contains('null', case=False))]\n",
        "print(\"len after data transformation:\", len(log_data))\n",
        "\n",
        "# Remove timestamp, [main], INFO, and file path from log messages\n",
        "cleaned_log_messages = []\n",
        "for log_message in log_messages:\n",
        "    cleaned_message = re.sub(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d+|\\[\\w+\\]|INFO|-\\s?[\\/\\w]+', '', log_message)\n",
        "    cleaned_log_messages.append(cleaned_message.strip())\n",
        "    \n",
        "# Convert log messages to TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(cleaned_log_messages)\n",
        "\n",
        "# Apply KNN for anomaly detection\n",
        "k = 5  # Number of neighbors to consider\n",
        "knn = NearestNeighbors(n_neighbors=k)\n",
        "knn.fit(X)\n",
        "\n",
        "# Find the k nearest neighbors for each log message\n",
        "distances, indices = knn.kneighbors(X)\n",
        "\n",
        "# Set a threshold for anomaly detection (you can adjust this as needed)\n",
        "threshold = 0\n",
        "\n",
        "# Determine the predicted labels (anomaly or normal)\n",
        "predicted_labels = [-1 if max(distances[i]) >= threshold else 1 for i in range(len(distances))]\n",
        "\n",
        "# Map true labels to numeric values (anomaly: -1, normal: 1)\n",
        "true_labels_numeric = [-1 if label == \"anomaly\" else 1 for label in true_labels]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_labels_numeric, predicted_labels)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(true_labels_numeric, predicted_labels, pos_label=-1)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(true_labels_numeric, predicted_labels, pos_label=-1)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(true_labels_numeric, predicted_labels, pos_label=-1)\n",
        "\n",
        "# Print the metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka71PczidNr-",
        "outputId": "6613a5b0-6dc7-4fa8-89e1-b8ba57be0c61"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "len before data transformation: 5000\n",
            "len after data transformation: 3609\n",
            "Accuracy: 0.2174\n",
            "Precision: 0.2174\n",
            "Recall: 1.0\n",
            "F1 score: 0.35715459175291603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Add User with One CLass SVM algo "
      ],
      "metadata": {
        "id": "csTyhrGOf5vR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Step 1: Load and preprocess the test data\n",
        "log_data = pd.read_csv('https://raw.githubusercontent.com/divanshu1993/security-anomalies-samples/main/fake_logs_5000.csv')\n",
        "\n",
        "# Preprocess the log data and extract the log messages and status\n",
        "log_messages = log_data['message'].tolist()\n",
        "status = log_data['status'].tolist()\n",
        "\n",
        "# Skip rows with null first name and last name\n",
        "print(\"len before data transformation:\", len(log_data))\n",
        "log_data = log_data[~(log_data['message'].astype(str).str.contains('First Name', case=False) & log_data['message'].astype(str).str.contains('null', case=False)) & ~(log_data['message'].astype(str).str.contains('Last Name', case=False) & log_data['message'].astype(str).str.contains('null', case=False))]\n",
        "print(\"len after data transformation:\", len(log_data))\n",
        "\n",
        "# Remove timestamp, [main], INFO, and file path from log messages\n",
        "cleaned_log_messages = []\n",
        "for log_message in log_messages:\n",
        "    cleaned_message = re.sub(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d+|\\[\\w+\\]|INFO|-\\s?[\\/\\w]+', '', log_message)\n",
        "    cleaned_log_messages.append(cleaned_message.strip())\n",
        "\n",
        "    \n",
        "\n",
        "# Convert log messages to TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(cleaned_log_messages)\n",
        "\n",
        "# Apply One-Class SVM for anomaly detection\n",
        "svm = OneClassSVM(nu=0.01)\n",
        "svm.fit(X)\n",
        "\n",
        "# Predict the anomaly scores for the log messages\n",
        "anomaly_scores = svm.decision_function(X)\n",
        "\n",
        "# Set a threshold for anomaly detection (you can adjust this as needed)\n",
        "threshold = 0\n",
        "\n",
        "# Determine the predicted labels (anomaly or normal)\n",
        "predicted_labels = [1 if score >= threshold else -1 for score in anomaly_scores]\n",
        "\n",
        "# Convert the status column to labels (-1 for anomaly, 1 for normal)\n",
        "ground_truth_labels = [-1 if s == 'anomaly' else 1 for s in status]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(ground_truth_labels, predicted_labels)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(ground_truth_labels, predicted_labels, pos_label=-1)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(ground_truth_labels, predicted_labels, pos_label=-1)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, pos_label=-1)\n",
        "\n",
        "# Print the metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 score:\", f1)\n",
        "\n",
        "# Add prediction column to the original dataframe\n",
        "log_data['prediction'] = predicted_labels\n",
        "\n",
        "# Save the dataframe with predictions to a CSV file\n",
        "log_data.to_csv('log_data_with_predictions.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "GWtLit_If57o",
        "outputId": "7921022b-e9b9-40a5-bc79-ef3809d72823"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "len before data transformation: 5000\n",
            "len after data transformation: 3609\n",
            "Accuracy: 0.6274\n",
            "Precision: 0.011335012594458438\n",
            "Recall: 0.00827966881324747\n",
            "F1 score: 0.009569377990430622\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-b34ff214b96e>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Add prediction column to the original dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mlog_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Save the dataframe with predictions to a CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3978\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3979\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3980\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3982\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4172\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4173\u001b[0m         \"\"\"\n\u001b[0;32m-> 4174\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4176\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4915\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4916\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \"\"\"\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    572\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of values (5000) does not match length of index (3609)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Add User with K Mean Clustering algo "
      ],
      "metadata": {
        "id": "ZsbJrEQniq0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import re\n",
        "\n",
        "# Step 4: Load and preprocess the test data\n",
        "log_data = pd.read_csv('https://raw.githubusercontent.com/divanshu1993/security-anomalies-samples/main/fake_logs_5000.csv')\n",
        "\n",
        "# Extract the log messages and status from the log_data DataFrame\n",
        "log_messages = log_data['message'].tolist()\n",
        "status_labels = log_data['status'].tolist()\n",
        "\n",
        "# Skip rows with null first name and last name\n",
        "print(\"len before data transformation:\", len(log_data))\n",
        "log_data = log_data[~(log_data['message'].astype(str).str.contains('First Name', case=False) & log_data['message'].astype(str).str.contains('null', case=False)) & ~(log_data['message'].astype(str).str.contains('Last Name', case=False) & log_data['message'].astype(str).str.contains('null', case=False))]\n",
        "print(\"len after data transformation:\", len(log_data))\n",
        "\n",
        "# Remove timestamp, [main], INFO, and file path from log messages\n",
        "cleaned_log_messages = []\n",
        "for log_message in log_messages:\n",
        "    cleaned_message = re.sub(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d+|\\[\\w+\\]|INFO|-\\s?[\\/\\w]+', '', log_message)\n",
        "    cleaned_log_messages.append(cleaned_message.strip())\n",
        "\n",
        "# Convert log messages to TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(cleaned_log_messages)\n",
        "\n",
        "# Apply K-means clustering for anomaly detection\n",
        "kmeans = KMeans(n_clusters=2)  # Assuming 2 clusters, one for normal data and one for anomalies\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Get the cluster labels for the log messages\n",
        "cluster_labels = kmeans.labels_\n",
        "\n",
        "# Map the status labels to the cluster labels\n",
        "status_mapping = {status: label for status, label in zip(status_labels, cluster_labels)}\n",
        "\n",
        "# Determine the predicted labels (anomaly or normal)\n",
        "predicted_labels = [-1 if status == 'anomaly' else 1 for status in status_labels]\n",
        "\n",
        "# Assume all log messages are normal\n",
        "ground_truth_labels = [1] * len(cleaned_log_messages)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(ground_truth_labels, predicted_labels)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(ground_truth_labels, predicted_labels, pos_label=-1)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(ground_truth_labels, predicted_labels, pos_label=-1)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(ground_truth_labels, predicted_labels, pos_label=-1)\n",
        "\n",
        "# Print the metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3785QBdirDM",
        "outputId": "28cae2b0-8acc-4539-b021-0cc70ef6f382"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len before data transformation: 5000\n",
            "len after data transformation: 3609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7826\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1 score: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generate Fake content for add users**"
      ],
      "metadata": {
        "id": "VVfaoSb4YXqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker\n",
        "import csv\n",
        "from faker import Faker\n",
        "import random\n",
        "\n",
        "fake = Faker('en_IN')\n",
        "\n",
        "# Open the CSV file in write mode\n",
        "with open('fake_logs_5000.csv', mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"message\", \"status\"])  # Add the \"status\" column header\n",
        "    for _ in range(5000):\n",
        "        date_time = fake.date_time_this_year()\n",
        "        # Generate empty first name for some entries\n",
        "        if random.random() <= 0.1:  # Adjust the probability as needed\n",
        "            first_name = 'null'\n",
        "        else:\n",
        "            first_name = fake.first_name()\n",
        "        \n",
        "        # Generate last name with <script></script> tags for some entries\n",
        "        if random.random() <= 0.1:  # Adjust the probability as needed\n",
        "            last_name = 'null'\n",
        "        elif random.random() <= 0.2:  # Adjust the probability as needed\n",
        "            last_name = 'CONNECT 172.65.27.22 HOME'\n",
        "        elif random.random() <= 0.1:  # Adjust the probability as needed\n",
        "            last_name = '<IMG src=\"..\"/>'\n",
        "        else:\n",
        "            last_name = fake.last_name()\n",
        "\n",
        "        if random.random() <= 0.1:  # Adjust the probability as needed\n",
        "            last_name = 'null'\n",
        "            first_name = 'null'\n",
        "\n",
        "\n",
        "        email = fake.email()\n",
        "        age = fake.random_int(min=18, max=70)\n",
        "        log_entry = f\"{date_time} [main] INFO com.example.UserService - Adding a new user: First Name={first_name}, Last Name={last_name}, Email={email}, Age={age}\"\n",
        "        \n",
        "        # Determine the status based on the conditions\n",
        "        if \"<IMG\" in last_name or \"CONNECT\" in last_name:\n",
        "            status = \"anomaly\"\n",
        "        else:\n",
        "            status = \"normal\"\n",
        "        \n",
        "        # Write the log entry and status as a row in the CSV file\n",
        "        writer.writerow([log_entry, status])\n",
        "\n",
        "# Print a message to indicate that the CSV file has been generated\n",
        "print(\"CSV file generated successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTzPcKzDF7h3",
        "outputId": "4d868a58-0f58-4eb0-e26e-08b290f444f4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faker\n",
            "  Downloading Faker-18.9.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Installing collected packages: faker\n",
            "Successfully installed faker-18.9.0\n",
            "CSV file generated successfully!\n"
          ]
        }
      ]
    }
  ]
}